{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Performance Prediction Models\n",
    "\n",
    "This notebook develops and validates predictive models for Bangladesh student educational outcomes:\n",
    "- Academic performance prediction\n",
    "- Dropout risk identification\n",
    "- Grade progression modeling\n",
    "- Intervention effectiveness prediction\n",
    "- Feature importance analysis\n",
    "- Model interpretability and validation\n",
    "\n",
    "**Machine Learning Models Include:**\n",
    "- Linear and polynomial regression\n",
    "- Random Forest and Gradient Boosting\n",
    "- Support Vector Machines\n",
    "- Neural Networks\n",
    "- Ensemble methods\n",
    "- Model evaluation and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "\n",
    "# Add project root to Python path\n",
    "sys.path.append('../..')\n",
    "from src.data_processing.data_processor import DataProcessor\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette('viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create comprehensive dataset for modeling\n",
    "np.random.seed(42)\n",
    "n_students = 5000\n",
    "\n",
    "# Generate synthetic student data with realistic relationships\n",
    "modeling_data = pd.DataFrame({\n",
    "    'student_id': [f'S{i:04d}' for i in range(1, n_students + 1)],\n",
    "    'division': np.random.choice(['Dhaka', 'Chittagong', 'Khulna', 'Rajshahi', 'Sylhet', 'Barishal', 'Rangpur', 'Mymensingh'], n_students),\n",
    "    'gender': np.random.choice(['Male', 'Female'], n_students),\n",
    "    'age': np.random.randint(13, 19, n_students),\n",
    "    'grade_level': np.random.choice([6, 7, 8, 9, 10, 11, 12], n_students),\n",
    "    'institution_type': np.random.choice(['Government', 'Private', 'Madrasa'], n_students, p=[0.65, 0.25, 0.10]),\n",
    "    'socioeconomic_status': np.random.choice(['Low', 'Medium', 'High'], n_students, p=[0.45, 0.35, 0.20]),\n",
    "    'area_type': np.random.choice(['Urban', 'Rural'], n_students, p=[0.35, 0.65]),\n",
    "    'mother_education': np.random.choice(['No Education', 'Primary', 'Secondary', 'Higher Secondary', 'University'], n_students, p=[0.3, 0.25, 0.25, 0.15, 0.05]),\n",
    "    'father_education': np.random.choice(['No Education', 'Primary', 'Secondary', 'Higher Secondary', 'University'], n_students, p=[0.25, 0.25, 0.25, 0.15, 0.10]),\n",
    "    'family_income': np.random.choice(['Very Low', 'Low', 'Medium', 'High', 'Very High'], n_students, p=[0.25, 0.30, 0.25, 0.15, 0.05]),\n",
    "    'previous_year_gpa': np.random.uniform(1.5, 4.5, n_students),\n",
    "    'days_absent_last_term': np.random.poisson(5, n_students),\n",
    "    'extracurricular_activities': np.random.choice([0, 1, 2, 3], n_students, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "    'home_internet': np.random.choice([0, 1], n_students, p=[0.6, 0.4]),\n",
    "    'study_hours_per_day': np.random.uniform(1, 8, n_students),\n",
    "    'teacher_student_ratio': np.random.uniform(25, 60, n_students),\n",
    "    'school_infrastructure_score': np.random.uniform(2, 10, n_students)\n",
    "})\n",
    "\n",
    "# Create target variable with realistic relationships\n",
    "# Base GPA influenced by multiple factors\n",
    "base_gpa = 2.5 + (modeling_data['previous_year_gpa'] - 2.5) * 0.7  # Strong correlation with previous GPA\n",
    "\n",
    "# Add effects of various factors\n",
    "ses_effect = modeling_data['socioeconomic_status'].map({'Low': -0.3, 'Medium': 0, 'High': 0.4})\n",
    "area_effect = modeling_data['area_type'].map({'Rural': -0.2, 'Urban': 0.2})\n",
    "inst_effect = modeling_data['institution_type'].map({'Government': 0, 'Private': 0.3, 'Madrasa': -0.1})\n",
    "gender_effect = modeling_data['gender'].map({'Male': 0, 'Female': 0.1})\n",
    "\n",
    "# Education effects\n",
    "edu_mapping = {'No Education': 0, 'Primary': 0.1, 'Secondary': 0.2, 'Higher Secondary': 0.3, 'University': 0.4}\n",
    "mother_edu_effect = modeling_data['mother_education'].map(edu_mapping) * 0.2\n",
    "father_edu_effect = modeling_data['father_education'].map(edu_mapping) * 0.15\n",
    "\n",
    "# Other effects\n",
    "attendance_effect = -modeling_data['days_absent_last_term'] * 0.02\n",
    "study_effect = modeling_data['study_hours_per_day'] * 0.05\n",
    "extracurricular_effect = modeling_data['extracurricular_activities'] * 0.1\n",
    "internet_effect = modeling_data['home_internet'] * 0.15\n",
    "infrastructure_effect = (modeling_data['school_infrastructure_score'] - 6) * 0.05\n",
    "\n",
    "# Combine all effects with some noise\n",
    "noise = np.random.normal(0, 0.3, n_students)\n",
    "\n",
    "modeling_data['current_gpa'] = (\n",
    "    base_gpa + ses_effect + area_effect + inst_effect + gender_effect +\n",
    "    mother_edu_effect + father_edu_effect + attendance_effect + \n",
    "    study_effect + extracurricular_effect + internet_effect + \n",
    "    infrastructure_effect + noise\n",
    ").clip(0, 5)\n",
    "\n",
    "# Create dropout risk indicator\n",
    "dropout_prob = (\n",
    "    0.05 +  # Base dropout rate\n",
    "    (modeling_data['days_absent_last_term'] > 15) * 0.3 +  # High absenteeism\n",
    "    (modeling_data['current_gpa'] < 2.0) * 0.4 +  # Low performance\n",
    "    (modeling_data['socioeconomic_status'] == 'Low') * 0.2 +  # Economic factors\n",
    "    (modeling_data['area_type'] == 'Rural') * 0.1  # Geographic factors\n",
    ")\n",
    "\n",
    "modeling_data['dropout_risk'] = np.random.binomial(1, dropout_prob.clip(0, 1), n_students)\n",
    "\n",
    "print(f\"Dataset created: {modeling_data.shape}\")\n",
    "print(f\"Target variable (current_gpa) statistics:\")\n",
    "print(modeling_data['current_gpa'].describe())\n",
    "print(f\"\\nDropout risk rate: {modeling_data['dropout_risk'].mean():.2%}\")\n",
    "\n",
    "modeling_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "def create_features(df):\n",
    "    \"\"\"Create additional features for modeling.\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Numerical features\n",
    "    df_features['age_grade_ratio'] = df_features['age'] / df_features['grade_level']\n",
    "    df_features['gpa_improvement_potential'] = 5.0 - df_features['previous_year_gpa']\n",
    "    df_features['attendance_rate'] = 1 - (df_features['days_absent_last_term'] / 200)  # Assuming 200 school days\n",
    "    df_features['study_intensity'] = df_features['study_hours_per_day'] / df_features['age']\n",
    "    \n",
    "    # Categorical feature combinations\n",
    "    df_features['ses_area'] = df_features['socioeconomic_status'] + '_' + df_features['area_type']\n",
    "    df_features['gender_institution'] = df_features['gender'] + '_' + df_features['institution_type']\n",
    "    \n",
    "    # Parent education combined score\n",
    "    edu_score_map = {'No Education': 0, 'Primary': 1, 'Secondary': 2, 'Higher Secondary': 3, 'University': 4}\n",
    "    df_features['parent_education_score'] = (\n",
    "        df_features['mother_education'].map(edu_score_map) + \n",
    "        df_features['father_education'].map(edu_score_map)\n",
    "    )\n",
    "    \n",
    "    # Risk indicators\n",
    "    df_features['high_absenteeism'] = (df_features['days_absent_last_term'] > 15).astype(int)\n",
    "    df_features['low_previous_performance'] = (df_features['previous_year_gpa'] < 2.5).astype(int)\n",
    "    df_features['limited_resources'] = (\n",
    "        (df_features['home_internet'] == 0) & \n",
    "        (df_features['socioeconomic_status'] == 'Low')\n",
    "    ).astype(int)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply feature engineering\n",
    "modeling_features = create_features(modeling_data)\n",
    "\n",
    "# Prepare features for modeling\n",
    "def prepare_features(df, target_col):\n",
    "    \"\"\"Prepare features for machine learning.\"\"\"\n",
    "    # Separate numerical and categorical features\n",
    "    numerical_features = [\n",
    "        'age', 'grade_level', 'previous_year_gpa', 'days_absent_last_term',\n",
    "        'extracurricular_activities', 'home_internet', 'study_hours_per_day',\n",
    "        'teacher_student_ratio', 'school_infrastructure_score',\n",
    "        'age_grade_ratio', 'gpa_improvement_potential', 'attendance_rate',\n",
    "        'study_intensity', 'parent_education_score', 'high_absenteeism',\n",
    "        'low_previous_performance', 'limited_resources'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = [\n",
    "        'division', 'gender', 'institution_type', 'socioeconomic_status',\n",
    "        'area_type', 'mother_education', 'father_education', 'family_income',\n",
    "        'ses_area', 'gender_institution'\n",
    "    ]\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X_numerical = df[numerical_features]\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    X_categorical = pd.get_dummies(df[categorical_features], drop_first=True)\n",
    "    \n",
    "    # Combine features\n",
    "    X = pd.concat([X_numerical, X_categorical], axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    return X, y, numerical_features, list(X_categorical.columns)\n",
    "\n",
    "# Prepare features for GPA prediction\n",
    "X, y, num_features, cat_features = prepare_features(modeling_features, 'current_gpa')\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of numerical features: {len(num_features)}\")\n",
    "print(f\"Number of categorical features: {len(cat_features)}\")\n",
    "print(f\"\\nFeature names:\")\n",
    "print(f\"Numerical: {num_features}\")\n",
    "print(f\"Categorical (first 10): {cat_features[:10]}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values: {X.isnull().sum().sum()}\")\n",
    "print(f\"Target variable range: {y.min():.2f} to {y.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Development and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Scale only numerical features\n",
    "X_train_scaled[num_features] = scaler.fit_transform(X_train[num_features])\n",
    "X_test_scaled[num_features] = scaler.transform(X_test[num_features])\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR': SVR(kernel='rbf'),\n",
    "    'Neural Network': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Use scaled data for models that benefit from scaling\n",
    "    if name in ['SVR', 'Neural Network', 'Ridge Regression', 'Lasso Regression']:\n",
    "        X_train_model = X_train_scaled\n",
    "        X_test_model = X_test_scaled\n",
    "    else:\n",
    "        X_train_model = X_train\n",
    "        X_test_model = X_test\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_model, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train_model)\n",
    "    y_pred_test = model.predict(X_test_model)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_model, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'cv_r2_mean': cv_scores.mean(),\n",
    "        'cv_r2_std': cv_scores.std(),\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "    \n",
    "    trained_models[name] = model\n",
    "\n",
    "print(\"\\nModel training completed!\")\n",
    "\n",
    "# Create results summary\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "results_df = results_df.drop('predictions', axis=1)\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"Model Performance Summary:\")\n",
    "print(results_df.sort_values('test_r2', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualization of model performance\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# RÂ² scores comparison\n",
    "model_names = list(model_results.keys())\n",
    "train_r2_scores = [model_results[name]['train_r2'] for name in model_names]\n",
    "test_r2_scores = [model_results[name]['test_r2'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, train_r2_scores, width, label='Train RÂ²', alpha=0.8)\n",
    "ax1.bar(x + width/2, test_r2_scores, width, label='Test RÂ²', alpha=0.8)\n",
    "ax1.set_xlabel('Models')\n",
    "ax1.set_ylabel('RÂ² Score')\n",
    "ax1.set_title('Model Performance Comparison (RÂ²)')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "train_rmse_scores = [model_results[name]['train_rmse'] for name in model_names]\n",
    "test_rmse_scores = [model_results[name]['test_rmse'] for name in model_names]\n",
    "\n",
    "ax2.bar(x - width/2, train_rmse_scores, width, label='Train RMSE', alpha=0.8)\n",
    "ax2.bar(x + width/2, test_rmse_scores, width, label='Test RMSE', alpha=0.8)\n",
    "ax2.set_xlabel('Models')\n",
    "ax2.set_ylabel('RMSE')\n",
    "ax2.set_title('Model Performance Comparison (RMSE)')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_means = [model_results[name]['cv_r2_mean'] for name in model_names]\n",
    "cv_stds = [model_results[name]['cv_r2_std'] for name in model_names]\n",
    "\n",
    "ax3.bar(model_names, cv_means, yerr=cv_stds, capsize=5, alpha=0.8)\n",
    "ax3.set_xlabel('Models')\n",
    "ax3.set_ylabel('Cross-Validation RÂ²')\n",
    "ax3.set_title('Cross-Validation Performance')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction vs Actual scatter plot for best model\n",
    "best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['test_r2'])\n",
    "best_predictions = model_results[best_model_name]['predictions']\n",
    "\n",
    "ax4.scatter(y_test, best_predictions, alpha=0.6)\n",
    "ax4.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax4.set_xlabel('Actual GPA')\n",
    "ax4.set_ylabel('Predicted GPA')\n",
    "ax4.set_title(f'Best Model: {best_model_name}\\n(Test RÂ² = {model_results[best_model_name][\"test_r2\"]:.3f})')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Model ranking\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "print(f\"Test RÂ²: {model_results[best_model_name]['test_r2']:.4f}\")\n",
    "print(f\"Test RMSE: {model_results[best_model_name]['test_rmse']:.4f}\")\n",
    "print(f\"Test MAE: {model_results[best_model_name]['test_mae']:.4f}\")\n",
    "\n",
    "# Calculate prediction intervals\n",
    "residuals = y_test - best_predictions\n",
    "residual_std = np.std(residuals)\n",
    "\n",
    "print(f\"\\nPrediction Accuracy Analysis:\")\n",
    "print(f\"Residual standard deviation: {residual_std:.3f}\")\n",
    "print(f\"95% prediction interval: Â±{1.96 * residual_std:.3f} GPA points\")\n",
    "\n",
    "# Accuracy within thresholds\n",
    "within_01 = np.mean(np.abs(residuals) <= 0.1) * 100\n",
    "within_02 = np.mean(np.abs(residuals) <= 0.2) * 100\n",
    "within_03 = np.mean(np.abs(residuals) <= 0.3) * 100\n",
    "\n",
    "print(f\"Predictions within 0.1 GPA points: {within_01:.1f}%\")\n",
    "print(f\"Predictions within 0.2 GPA points: {within_02:.1f}%\")\n",
    "print(f\"Predictions within 0.3 GPA points: {within_03:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance analysis for tree-based models\n",
    "def analyze_feature_importance(model, model_name, feature_names, X_test, y_test):\n",
    "    \"\"\"Analyze feature importance using multiple methods.\"\"\"\n",
    "    importance_data = {}\n",
    "    \n",
    "    # Built-in feature importance (for tree-based models)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_data['built_in'] = model.feature_importances_\n",
    "    \n",
    "    # Permutation importance\n",
    "    perm_importance = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "    importance_data['permutation'] = perm_importance.importances_mean\n",
    "    \n",
    "    return importance_data\n",
    "\n",
    "# Analyze feature importance for the best models\n",
    "top_models = ['Random Forest', 'Gradient Boosting']\n",
    "\n",
    "feature_importance_results = {}\n",
    "\n",
    "for model_name in top_models:\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        \n",
    "        # Use appropriate test data\n",
    "        if model_name in ['SVR', 'Neural Network', 'Ridge Regression', 'Lasso Regression']:\n",
    "            X_test_model = X_test_scaled\n",
    "        else:\n",
    "            X_test_model = X_test\n",
    "        \n",
    "        importance_data = analyze_feature_importance(model, model_name, X.columns, X_test_model, y_test)\n",
    "        feature_importance_results[model_name] = importance_data\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(len(top_models), 2, figsize=(15, 6*len(top_models)))\n",
    "if len(top_models) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, model_name in enumerate(top_models):\n",
    "    if model_name in feature_importance_results:\n",
    "        importance_data = feature_importance_results[model_name]\n",
    "        \n",
    "        # Built-in importance\n",
    "        if 'built_in' in importance_data:\n",
    "            built_in_importance = importance_data['built_in']\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'feature': X.columns,\n",
    "                'importance': built_in_importance\n",
    "            }).sort_values('importance', ascending=True).tail(15)\n",
    "            \n",
    "            axes[i][0].barh(feature_importance_df['feature'], feature_importance_df['importance'])\n",
    "            axes[i][0].set_title(f'{model_name} - Built-in Feature Importance')\n",
    "            axes[i][0].set_xlabel('Importance')\n",
    "        \n",
    "        # Permutation importance\n",
    "        perm_importance = importance_data['permutation']\n",
    "        perm_importance_df = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': perm_importance\n",
    "        }).sort_values('importance', ascending=True).tail(15)\n",
    "        \n",
    "        axes[i][1].barh(perm_importance_df['feature'], perm_importance_df['importance'])\n",
    "        axes[i][1].set_title(f'{model_name} - Permutation Importance')\n",
    "        axes[i][1].set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top features summary\n",
    "print(\"TOP FEATURES ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name in top_models:\n",
    "    if model_name in feature_importance_results:\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        \n",
    "        # Permutation importance (more reliable)\n",
    "        perm_importance = feature_importance_results[model_name]['permutation']\n",
    "        top_features = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': perm_importance\n",
    "        }).sort_values('importance', ascending=False).head(10)\n",
    "        \n",
    "        print(\"Top 10 features (Permutation Importance):\")\n",
    "        for idx, row in top_features.iterrows():\n",
    "            print(f\"  {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dropout Risk Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Develop dropout risk prediction model\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Prepare data for dropout prediction\n",
    "X_dropout, y_dropout, _, _ = prepare_features(modeling_features, 'dropout_risk')\n",
    "\n",
    "# Split data\n",
    "X_train_dropout, X_test_dropout, y_train_dropout, y_test_dropout = train_test_split(\n",
    "    X_dropout, y_dropout, test_size=0.2, random_state=42, stratify=y_dropout\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "X_train_dropout_scaled = X_train_dropout.copy()\n",
    "X_test_dropout_scaled = X_test_dropout.copy()\n",
    "X_train_dropout_scaled[num_features] = scaler.fit_transform(X_train_dropout[num_features])\n",
    "X_test_dropout_scaled[num_features] = scaler.transform(X_test_dropout[num_features])\n",
    "\n",
    "# Define classification models\n",
    "dropout_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate dropout models\n",
    "dropout_results = {}\n",
    "\n",
    "for name, model in dropout_models.items():\n",
    "    print(f\"Training {name} for dropout prediction...\")\n",
    "    \n",
    "    # Use scaled data for logistic regression\n",
    "    if name == 'Logistic Regression':\n",
    "        X_train_model = X_train_dropout_scaled\n",
    "        X_test_model = X_test_dropout_scaled\n",
    "    else:\n",
    "        X_train_model = X_train_dropout\n",
    "        X_test_model = X_test_dropout\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_model, y_train_dropout)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_dropout = model.predict(X_test_model)\n",
    "    y_pred_proba_dropout = model.predict_proba(X_test_model)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc_score = roc_auc_score(y_test_dropout, y_pred_proba_dropout)\n",
    "    \n",
    "    dropout_results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred_dropout,\n",
    "        'probabilities': y_pred_proba_dropout,\n",
    "        'auc_score': auc_score\n",
    "    }\n",
    "\n",
    "# Find best dropout model\n",
    "best_dropout_model = max(dropout_results.keys(), key=lambda x: dropout_results[x]['auc_score'])\n",
    "\n",
    "print(f\"\\nBest dropout prediction model: {best_dropout_model}\")\n",
    "print(f\"AUC Score: {dropout_results[best_dropout_model]['auc_score']:.4f}\")\n",
    "\n",
    "# Detailed evaluation of best model\n",
    "best_model = dropout_results[best_dropout_model]['model']\n",
    "best_predictions = dropout_results[best_dropout_model]['predictions']\n",
    "best_probabilities = dropout_results[best_dropout_model]['probabilities']\n",
    "\n",
    "print(f\"\\nClassification Report for {best_dropout_model}:\")\n",
    "print(classification_report(y_test_dropout, best_predictions))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_dropout, best_predictions)\n",
    "\n",
    "# Visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# AUC comparison\n",
    "model_names_dropout = list(dropout_results.keys())\n",
    "auc_scores = [dropout_results[name]['auc_score'] for name in model_names_dropout]\n",
    "\n",
    "ax1.bar(model_names_dropout, auc_scores, color='skyblue', alpha=0.8)\n",
    "ax1.set_xlabel('Models')\n",
    "ax1.set_ylabel('AUC Score')\n",
    "ax1.set_title('Dropout Prediction Model Comparison')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2)\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "ax2.set_title(f'Confusion Matrix - {best_dropout_model}')\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test_dropout, best_probabilities)\n",
    "ax3.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {dropout_results[best_dropout_model][\"auc_score\"]:.3f})')\n",
    "ax3.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "ax3.set_xlabel('False Positive Rate')\n",
    "ax3.set_ylabel('True Positive Rate')\n",
    "ax3.set_title('ROC Curve')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Dropout probability distribution\n",
    "ax4.hist(best_probabilities[y_test_dropout == 0], bins=30, alpha=0.7, label='Not at Risk', density=True)\n",
    "ax4.hist(best_probabilities[y_test_dropout == 1], bins=30, alpha=0.7, label='At Risk', density=True)\n",
    "ax4.set_xlabel('Dropout Probability')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.set_title('Dropout Probability Distribution')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Ensemble and Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create ensemble model for GPA prediction\n",
    "ensemble_models = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
    "    ('ridge', Ridge(alpha=1.0))\n",
    "]\n",
    "\n",
    "ensemble_regressor = VotingRegressor(ensemble_models)\n",
    "ensemble_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Ensemble predictions\n",
    "ensemble_predictions = ensemble_regressor.predict(X_test)\n",
    "ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_predictions))\n",
    "\n",
    "print(f\"Ensemble Model Performance:\")\n",
    "print(f\"RÂ² Score: {ensemble_r2:.4f}\")\n",
    "print(f\"RMSE: {ensemble_rmse:.4f}\")\n",
    "\n",
    "# Compare with best individual model\n",
    "best_individual_r2 = model_results[best_model_name]['test_r2']\n",
    "improvement = ensemble_r2 - best_individual_r2\n",
    "\n",
    "print(f\"\\nComparison with best individual model ({best_model_name}):\")\n",
    "print(f\"Individual model RÂ²: {best_individual_r2:.4f}\")\n",
    "print(f\"Ensemble model RÂ²: {ensemble_r2:.4f}\")\n",
    "print(f\"Improvement: {improvement:+.4f}\")\n",
    "\n",
    "# Create prediction function\n",
    "def predict_student_performance(student_features, gpa_model, dropout_model, scaler, feature_columns):\n",
    "    \"\"\"Predict student GPA and dropout risk.\"\"\"\n",
    "    # Ensure features are in correct order\n",
    "    student_df = pd.DataFrame([student_features])\n",
    "    \n",
    "    # Apply feature engineering\n",
    "    student_engineered = create_features(student_df)\n",
    "    \n",
    "    # Prepare features\n",
    "    X_student, _, _, _ = prepare_features(student_engineered, 'current_gpa')\n",
    "    \n",
    "    # Ensure all columns are present\n",
    "    for col in feature_columns:\n",
    "        if col not in X_student.columns:\n",
    "            X_student[col] = 0\n",
    "    \n",
    "    X_student = X_student[feature_columns]\n",
    "    \n",
    "    # Scale numerical features\n",
    "    X_student_scaled = X_student.copy()\n",
    "    X_student_scaled[num_features] = scaler.transform(X_student[num_features])\n",
    "    \n",
    "    # Make predictions\n",
    "    gpa_prediction = gpa_model.predict(X_student)[0]\n",
    "    dropout_risk = dropout_model.predict_proba(X_student_scaled)[:, 1][0]\n",
    "    \n",
    "    return gpa_prediction, dropout_risk\n",
    "\n",
    "# Example prediction\n",
    "example_student = {\n",
    "    'student_id': 'S_NEW_001',\n",
    "    'division': 'Dhaka',\n",
    "    'gender': 'Female',\n",
    "    'age': 16,\n",
    "    'grade_level': 10,\n",
    "    'institution_type': 'Government',\n",
    "    'socioeconomic_status': 'Medium',\n",
    "    'area_type': 'Urban',\n",
    "    'mother_education': 'Secondary',\n",
    "    'father_education': 'Higher Secondary',\n",
    "    'family_income': 'Medium',\n",
    "    'previous_year_gpa': 3.5,\n",
    "    'days_absent_last_term': 8,\n",
    "    'extracurricular_activities': 2,\n",
    "    'home_internet': 1,\n",
    "    'study_hours_per_day': 4.5,\n",
    "    'teacher_student_ratio': 35,\n",
    "    'school_infrastructure_score': 7.5\n",
    "}\n",
    "\n",
    "# Get best models\n",
    "best_gpa_model = ensemble_regressor\n",
    "best_dropout_model_obj = dropout_results[best_dropout_model]['model']\n",
    "\n",
    "# Make prediction\n",
    "predicted_gpa, predicted_dropout_risk = predict_student_performance(\n",
    "    example_student, best_gpa_model, best_dropout_model_obj, scaler, X.columns\n",
    ")\n",
    "\n",
    "print(f\"\\nExample Student Prediction:\")\n",
    "print(f\"Student Profile: {example_student['gender']}, {example_student['age']} years old, Grade {example_student['grade_level']}\")\n",
    "print(f\"Previous GPA: {example_student['previous_year_gpa']}\")\n",
    "print(f\"\\nPredictions:\")\n",
    "print(f\"Predicted GPA: {predicted_gpa:.2f}\")\n",
    "print(f\"Dropout Risk: {predicted_dropout_risk:.1%}\")\n",
    "\n",
    "# Risk categorization\n",
    "if predicted_dropout_risk < 0.1:\n",
    "    risk_level = 'Low'\n",
    "elif predicted_dropout_risk < 0.3:\n",
    "    risk_level = 'Medium'\n",
    "else:\n",
    "    risk_level = 'High'\n",
    "\n",
    "print(f\"Risk Level: {risk_level}\")\n",
    "\n",
    "# Save models\n",
    "model_save_path = Path('../../models')\n",
    "model_save_path.mkdir(exist_ok=True)\n",
    "\n",
    "joblib.dump(best_gpa_model, model_save_path / 'gpa_prediction_model.pkl')\n",
    "joblib.dump(best_dropout_model_obj, model_save_path / 'dropout_prediction_model.pkl')\n",
    "joblib.dump(scaler, model_save_path / 'feature_scaler.pkl')\n",
    "\n",
    "print(f\"\\nModels saved to {model_save_path}:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate comprehensive insights\n",
    "print(\"ğŸ¯ PREDICTIVE MODELING INSIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Model performance summary\n",
    "print(f\"ğŸ“Š Model Performance Summary:\")\n",
    "print(f\"   â€¢ Best GPA prediction model: Ensemble (RÂ² = {ensemble_r2:.3f})\")\n",
    "print(f\"   â€¢ Best dropout prediction model: {best_dropout_model} (AUC = {dropout_results[best_dropout_model]['auc_score']:.3f})\")\n",
    "print(f\"   â€¢ Prediction accuracy within 0.2 GPA: {within_02:.1f}%\")\n",
    "\n",
    "# Feature insights\n",
    "print(f\"ğŸ” Key Predictive Factors:\")\n",
    "\n",
    "if 'Random Forest' in feature_importance_results:\n",
    "    rf_importance = feature_importance_results['Random Forest']['permutation']\n",
    "    top_predictors = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf_importance\n",
    "    }).sort_values('importance', ascending=False).head(5)\n",
    "    \n",
    "    print(\"   Top 5 predictors of academic performance:\")\n",
    "    for idx, row in top_predictors.iterrows():\n",
    "        feature_name = row['feature']\n",
    "        importance = row['importance']\n",
    "        print(f\"     â€¢ {feature_name}: {importance:.4f}\")\n",
    "\n",
    "# Risk factor analysis\n",
    "print(f\"âš ï¸ Dropout Risk Factors:\")\n",
    "high_risk_students = modeling_features[modeling_features['dropout_risk'] == 1]\n",
    "low_risk_students = modeling_features[modeling_features['dropout_risk'] == 0]\n",
    "\n",
    "# Compare characteristics\n",
    "risk_comparison = {\n",
    "    'Average GPA': {\n",
    "        'High Risk': high_risk_students['current_gpa'].mean(),\n",
    "        'Low Risk': low_risk_students['current_gpa'].mean()\n",
    "    },\n",
    "    'Average Absences': {\n",
    "        'High Risk': high_risk_students['days_absent_last_term'].mean(),\n",
    "        'Low Risk': low_risk_students['days_absent_last_term'].mean()\n",
    "    },\n",
    "    'Low SES %': {\n",
    "        'High Risk': (high_risk_students['socioeconomic_status'] == 'Low').mean() * 100,\n",
    "        'Low Risk': (low_risk_students['socioeconomic_status'] == 'Low').mean() * 100\n",
    "    }\n",
    "}\n",
    "\n",
    "for metric, values in risk_comparison.items():\n",
    "    print(f\"   â€¢ {metric}:\")\n",
    "    print(f\"     - High risk students: {values['High Risk']:.2f}\")\n",
    "    print(f\"     - Low risk students: {values['Low Risk']:.2f}\")\n",
    "\n",
    "# Intervention recommendations\n",
    "print(f\"ğŸ’¡ Evidence-Based Intervention Recommendations:\")\n",
    "print(f\"\nğŸ¯ Early Warning System:\")\n",
    "print(f\"   â€¢ Monitor students with GPA < 2.5 (high dropout risk)\")\n",
    "print(f\"   â€¢ Track attendance closely (>10 absences = increased risk)\")\n",
    "print(f\"   â€¢ Identify students with multiple risk factors\")\n",
    "\n",
    "print(f\"ğŸ‹ï¸ Targeted Interventions:\")\n",
    "print(f\"   â€¢ Academic support for students with previous GPA < 3.0\")\n",
    "print(f\"   â€¢ Attendance improvement programs\")\n",
    "print(f\"   â€¢ Family engagement initiatives for low SES students\")\n",
    "print(f\"   â€¢ Technology access programs (home internet/devices)\")\n",
    "\n",
    "print(f\"ğŸ“Š Performance Optimization:\")\n",
    "print(f\"   â€¢ Encourage study time of 4-6 hours per day\")\n",
    "print(f\"   â€¢ Promote extracurricular participation\")\n",
    "print(f\"   â€¢ Improve school infrastructure where needed\")\n",
    "print(f\"   â€¢ Reduce teacher-student ratios in underperforming areas\")\n",
    "\n",
    "print(f\"ğŸ“ Monitoring and Evaluation:\")\n",
    "print(f\"   â€¢ Implement quarterly prediction updates\")\n",
    "print(f\"   â€¢ Track intervention effectiveness\")\n",
    "print(f\"   â€¢ Adjust models based on new data\")\n",
    "print(f\"   â€¢ Validate predictions against actual outcomes\")\n",
    "\n",
    "# Model limitations and future improvements\n",
    "print(f\"ğŸ”„ Model Limitations and Future Improvements:\")\n",
    "print(f\"   â€¢ Current model explains {ensemble_r2:.1%} of GPA variance\")\n",
    "print(f\"   â€¢ Consider adding psychological and social factors\")\n",
    "print(f\"   â€¢ Incorporate longitudinal tracking data\")\n",
    "print(f\"   â€¢ Add teacher quality and classroom environment variables\")\n",
    "print(f\"   â€¢ Develop subject-specific prediction models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
